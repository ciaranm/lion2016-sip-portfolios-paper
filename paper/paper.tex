\documentclass{llncs}

% \usepackage{showframe}

\usepackage{times}
\usepackage{complexity}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{xcolor,colortbl}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{xspace}
\usepackage{scalefnt}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[sort]{cite}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}

\newcommand{\VFtwoNS}{$\textsc{Vf{\scalefont{0.8}2}}$}
\newcommand{\GlasgowNS}{$\textsc{Glasgow}$}
\newcommand{\LADNS}{$\textsc{Lad}$}
\newcommand{\IncompleteLADNS}{$\textsc{IncompleteLad}$}
\newcommand{\PathLADNS}{$\textsc{PathLad}$}
\newcommand{\GlasgowOneNS}{$\textsc{Glasgow{\scalefont{0.8}1}}$}
\newcommand{\GlasgowTwoNS}{$\textsc{Glasgow{\scalefont{0.8}2}}$}
\newcommand{\GlasgowThreeNS}{$\textsc{Glasgow{\scalefont{0.8}3}}$}
\newcommand{\GlasgowFourNS}{$\textsc{Glasgow{\scalefont{0.8}4}}$}
\newcommand{\LLAMANS}{$\textsc{Llama}$}

\newcommand{\VFtwo}{$\textsc{Vf{\scalefont{0.8}2}}$\xspace}
\newcommand{\Glasgow}{$\textsc{Glasgow}$\xspace}
\newcommand{\LAD}{$\textsc{Lad}$\xspace}
\newcommand{\IncompleteLAD}{$\textsc{IncompleteLad}$\xspace}
\newcommand{\PathLAD}{$\textsc{PathLad}$\xspace}
\newcommand{\GlasgowOne}{$\textsc{Glasgow{\scalefont{0.8}1}}$\xspace}
\newcommand{\GlasgowTwo}{$\textsc{Glasgow{\scalefont{0.8}2}}$\xspace}
\newcommand{\GlasgowThree}{$\textsc{Glasgow{\scalefont{0.8}3}}$\xspace}
\newcommand{\GlasgowFour}{$\textsc{Glasgow{\scalefont{0.8}4}}$\xspace}
\newcommand{\LLAMA}{$\textsc{Llama}$\xspace}

\title{Portfolios of Subgraph Isomorphism Algorithms}

\author{
    Lars Kotthoff\thanks{This work was supported by an NSERC E.W.R.\ Steacie
    Fellowship and under the NSERC Discovery Grant Program.}\inst{1}
    \and Ciaran McCreesh\thanks{This work was supported by the Engineering
        and Physical Sciences Research Council [grant number EP/K503058/1]}\inst{2}
    \and Christine Solnon\thanks{This work has been supported by the ANR project SoLStiCe (ANR-13-BS02-0002-01).}\inst{3}}

\institute{
    University of British Columbia, Vancouver, Canada
    \and University of Glasgow, Glasgow, Scotland
    \and INSA-Lyon, LIRIS, UMR5205, F-69621, France}

\begin{document}

\maketitle

\begin{abstract}
Subgraph isomorphism is a computationally challenging problem with important practical
applications, for example in computer vision, biochemistry, and model checking. There are a number
of state-of-the-art algorithms for solving the problem, each of which has its own performance
characteristics. As with many other hard problems, the single best choice of algorithm overall is
rarely the best algorithm on an instance-by-instance. We develop an algorithm
selection approach which leverages
novel features to characterise subgraph isomorphism problems to dynamically decide which algorithm
to use on a per-instance basis. We demonstrate substantial performance improvements on a large set
of hard benchmark problems. In addition, we show how algorithm selection models can be leveraged to
gain new insights into what affects the performance of an algorithm.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to find an adjacency-preserving injective mapping from vertices
of a small \emph{pattern} graph to vertices of a large \emph{target} graph. This \NP-complete
problem has many important practical applications, for example in computer
vision~\cite{cviu11,pr15}, biochemistry~\cite{Giugno:2013}, and model checking~\cite{Sevegnani:2015}. There
exist various exact algorithms, which have been compared on a large suite of problem instances by
McCreesh and Prosser~\cite{McCreesh:2015}. These experiments indicated that the single best
algorithm depends on the CPU time limit considered: for very small CPU time
limits, \VFtwo{}~\cite{Cordella:2004} is the best choice, whereas the \Glasgow algorithm~\cite{McCreesh:2015} has
better success rates for larger time limits.  They also showed that on an instance by instance
basis, other algorithms are often better.


The per-instance algorithm selection problem~\cite{rice_algorithm_1976} is to select from an
algorithm portfolio~\cite{huberman_economics_1997,gomes_algorithm_2001} the
algorithm expected to perform
best on a given problem instance. Algorithm selection systems usually build machine learning models
of the algorithms or the portfolio which they are contained in to forecast which algorithm to use in a
particular context. Using the predictions, one or more algorithms from the
portfolio can be selected to be run sequentially or in parallel.

Here, we consider the case where exactly one algorithm is selected for solving the problem. One of
the most prominent and successful systems that employs this approach is
SATzilla~\cite{xu_satzilla_2008}, which defined the state of the art in SAT solving for a number of
years. Other application areas include constraint solving~\cite{omahony_using_2008}, the travelling
salesperson problem~\cite{kotthoff_improving_2015}, and AI planning~\cite{seipp_learning_2012}.
The interested reader is referred to a recent survey~\cite{kotthoff_algorithm_2014} for additional
information on algorithm selection.

\paragraph{Overview of the paper.}
We formally define the subgraph isomorphism problem in Section~\ref{sec:defs}.
In Section~\ref{sec:algs}, we describe the main existing algorithms for solving
this problem, and we also introduce two new algorithms which are derived from
Solnon's \LAD algorithm \cite{Solnon:2010}.

In Section~\ref{sec:exps}, we experimentally compare eight state-of-the-art
algorithms. We introduce a large benchmark set composed of 5725 instances
grouped into twelve classes. Ten of these classes were considered in the
experimental study reported by McCreesh and Prosser~\cite{McCreesh:2015}; two
are new. We evaluate the algorithms on this benchmark set, and show that they
have very complementary performance. In particular, we show that depending on
the CPU time limit, different algorithms achieve the best performance on the
entire benchmark set.

In Section~\ref{sec:algsel}, we discuss the features that are used to describe
instances, and we describe our algorithm selection approach. It combines a
presolving step, which allows us to easy instances very quickly, with the
algorithm selection approach \LLAMA~\cite{kotthoff_llama_2013}.
In Section~\ref{sec:algsel-exps}, we experimentally evaluate our selection
approach, and show that it is able to close more than \SI{60}{\percent} of the
gap between the single best and the virtual best solver. We conclude and give
directions for future work in Section~\ref{sec:concs}.

\section{Definitions and Notations}\label{sec:defs}

A \emph{graph} $G=(N,E)$ consists of a \emph{node set} $N$ and an \emph{edge set} $E \subseteq N
\times N$, where an edge $(u,u')$ is a pair of nodes. The number of neighbors of a node $u$ is
called the degree of $u$, denoted $d^\circ(u)=\#\{ (u,u')\in E\}$. In this paper, we implicitly consider
non-directed graphs, such that $(u,u')\in E\Leftrightarrow (u',u)\in E$. The extension to directed
graphs is rather straightforward, and all algorithms compared in this paper can handle directed
graphs as well.

Given a pattern graph $G_p=(N_p,E_p)$ and a target graph $G_t=(N_t,E_t)$, the \emph{subgraph
isomorphism problem} consists of deciding whether $G_p$ is isomorphic to some subgraph of $G_t$.
More formally, the goal is to find an injective matching $f: N_p\rightarrow N_t$, that associates a
different target node to each pattern node, and preserves pattern edges, i.e.\ $\forall (u,u')
\in E_p, (f(u),f(u')) \in E_t$.
Note that the subgraph is not necessarily induced, so that two pattern nodes not linked by
an edge may be mapped to two target nodes which are linked by an edge.

In the following, we assume $G_p=(N_p,E_p)$ and $G_t=(N_t,E_t)$ to be the underlying instance of 
subgraph isomorphism.  We  define $n_p = \# N_p$, $n_t = \# N_t$,  $e_p=\# E_p$, $e_t=\#
E_t$, and $d_p$ and $d_t$ to be the maximum degrees of the graphs $G_p$ and $G_t$.

\section{Subgraph Isomorphism Algorithms}\label{sec:algs}

Subgraph isomorphism problems may be solved by a systematic exploration of the search space consisting
of all possible injective matchings from $N_p$ to $N_t$: starting from an empty matching, one
incrementally extends a partial matching by matching a non-matched pattern node to a non-matched
target node until either some edges are not matched by the current matching (so the search must
backtrack to a previous choice point and go on with another extension), or all pattern nodes have
been matched (a solution has been found). To reduce the search space, this exhaustive exploration is
combined with filtering techniques that aim at removing candidate pairs of non-matched
pattern-target nodes $(u,v)\in N_p\times N_t$. Different filtering techniques may be considered;
some are stronger than others (they remove more candidate pairs), but also have higher time
complexities.

\subsection{Filtering for Subgraph Isomorphism}

The simplest form of filtering is to propagate difference constraints (which ensure that the
matching is injective) and edge constraints (which ensure that the matching preserves pattern
edges): each time a pattern node $u\in N_p$ is matched with a target node $v\in N_t$, one removes
every candidate pair $(u',v')\in N_p\times N_t$ such that either $v'=v$ (difference constraint) or
$(u,u')$ is a pattern edge but $(v,v')$ is not a target edge (edge constraint). This simple
filtering (called \emph{Forward-Checking}) is very fast to achieve: in ${\cal O}(n_p)$ for
difference constraints, and in ${\cal O}(d_p\cdot n_t)$ for edge constraints. It is used, for
example, in McGregor's algorithm~\cite{mcgregor79} and in \VFtwo{}~\cite{Cordella:2004}.

R\'egin~\cite{regin} introduced a stronger filtering for difference constraints, which ensures that
all pattern nodes can be matched with different target nodes, all together. This filtering (called
\emph{All-Different Generalized Arc Consistency}) removes more candidate pairs than when each
difference constraint is propagated separately which Forward-Checking. However, it is also more time
consuming as it requires ${\cal O}(n_p^2\cdot n_t^2)$ time.

Various filtering techniques have been tried for edge constraints. Ullman~\cite{ullman} introduced a
filtering which ensures that for each pattern edge $(u,u')\in E_p$ and each candidate pair $(u,v)\in
N_p\times N_t$, there exists a candidate pair $(u',v')\in N_p\times N_t$ such that $(v,v')$ is a
target edge. Candidate pairs $(u,v)$ that do not satisfy this property are iteratively removed
until a fixed point is reached. This filtering (called \emph{Arc Consistency}) removes more
candidate pairs than Forward-Checking, but it is also more time consuming as it
runs in ${\cal
O}(e_p\cdot n_t^2)$ when using AC4~\cite{MH86}.

Stronger filtering may be obtained by propagating edge constraints in a more global way, as proposed
by Larrosa and Valiente~\cite{LV02}. The idea is to check for each candidate pair $(u,v)\in
N_p\times N_t$ that the number of pattern nodes adjacent to $u$ is smaller than or equal to the
number of target nodes that are both adjacent to $v$ and that may be matched with nodes adjacent to
$u$. This is done in ${\cal O}(n_p^2\cdot n_t^2)$. This idea was generalised by
Solnon's \LAD algorithm~\cite{Solnon:2010}, where, for each candidate pair $(u,v)\in N_p\times N_t$, a redundant Local
All-Different constraint ensures that each neighbour of $u$ may be matched with a different
neighbour of $v$. This is done in ${\cal O}(n_p\cdot n_t\cdot d_p^2\cdot d_t^2)$.

\subsection{Propagation of Invariant Properties}

Some filtering techniques exploit invariant properties, i.e.\ properties associated with nodes such
that nodes may be matched only if they have compatible properties. A classical property is the
degree: a pattern node $u\in N_p$ may be matched with a target node $v\in N_t$ only if
$d^\circ(u)\leq d^\circ(v)$. This property is usually used at the beginning of the search to reduce
the set of candidate pairs to $\{(u,v)\in N_p\times N_t\;|\;d^\circ(u)\leq d^\circ(v)\}$.  Other
examples of invariant properties are the number of cycles of length $k$ passing through the node,
and the number of cliques of size $k$ containing the node, which must be smaller for a pattern node
than for its matched target node.  Invariant properties may also be associated with pairs of nodes.
For example, the number of paths of length $k$ between two pattern nodes is smaller than or equal to
the number of paths of length $k$ between the target nodes with which they may be matched.
These invariant properties are used, for example,

\begin{itemize}[nosep]
\item by Battiti and Mascia~\cite{battiti-mascia07}, to remove candidate pairs $(u,v)\in N_p\times
    N_t$ such that the number of paths starting from  pattern node $u$ is greater than the number of
    paths starting from  target node $v$;
\item by Audemard et al.~\cite{Audemard:2014} to generalise the locally all-different constraint
    proposed by Solnon~\cite{Solnon:2010} so that it ensures that a subset of pattern nodes can be
    matched with all different compatible target nodes, where compatibility is defined with respect
    to invariant properties;
\item by McCreesh and Prosser~\cite{McCreesh:2015} to filter the set of candidate pairs before
    starting the search, and to generate additional implied adjacency-like constraints which are
    processed during search.
\end{itemize}

\noindent Audemard et al.~\cite{Audemard:2014} do not limit the length of paths considered, and
iteratively increment the length until no more pairs are removed. Battiti and Mascia~\cite{battiti-mascia07}, and McCreesh and Prosser~\cite{McCreesh:2015} parameterise their algorithms
by the maximum path length considered when counting paths: larger values for this parameter remove
more candidate pairs, but are also more time consuming. Battiti and Mascia's experiments show that
the best setting depends on the instance considered, and that a portfolio running several randomised
versions in time-sharing decreases the total CPU time needed to find a solution for feasible
instances. McCreesh and Prosser simply set the parameter to 3, as this setting
presented the best overall performance in their case.

\section{Experimental Comparison of Individual Algorithms}\label{sec:exps}

We consider six algorithms from the literature and propose two novel ones.

\subsection{Algorithms from the Literature}

We have selected a set of algorithms with complementary performance. These
algorithms will compose our portfolio in the per-instance selection approach
described in Section~\ref{sec:algsel}.
\begin{itemize}
\item \VFtwo{}~\cite{Cordella:2004} performs weak filtering that is especially fast on
    trivially satisfiable instances;
\item \LAD{}~\cite{Solnon:2010} combines two strong but expensive filtering techniques
    (All-Different Generalized Arc Consistency and Locally All-Different);
\item \Glasgow{}~\cite{McCreesh:2015} does expensive preprocessing based on path length
    invariant properties to generate additional constraints, followed by weaker filtering
    (forward-checking, and a heuristic All-Different propagator which can miss deletions) and
    conflict-directed backjumping during search.
\end{itemize}

\noindent The \Glasgow algorithm has a parameter, which controls the lengths of paths used when
reasoning about non-adjacent vertices.  In experiments reported by McCreesh and
Prosser~\cite{McCreesh:2015}, the choice of paths of length 3 was used as a reasonable compromise---longer
paths lead to prohibitively expensive preprocessing on larger, denser instances. This is often not
the best choice on an instance by instance basis: sometimes path-based reasoning gives no benefit at
all, sometimes considering only paths of length 2 suffices, occasionally paths of length 4 are
helpful, and even looking at paths of length 3 is relatively expensive on some graphs. We thus consider all
lengths up to 4, naming these variants \GlasgowOne through \GlasgowFour.

\subsection{New Algorithms}

We introduce two new variants of \LAD. The first, called \IncompleteLAD, does weaker
filtering which is applied once, without performing a backtracking search, and very quickly
detects inconsistencies on many instances: for each pattern node $u$, we check that there exists at
least one target node $v$ such that for each neighbor $u'$ of $u$ there exists a different neighbor
$v'$ of $v$ such that the degree of $u'$ is smaller than or equal to the degree of $v'$.
\IncompleteLAD is an incomplete algorithm that checks a sufficient, but not
necessary, condition for inconsistency: when it does not detect inconsistency,
the instance may still be unsatisfiable. Its main benefit is that it runs very
fast: its time complexity is ${\cal O}(n_p(n_t+e_t))$.

The second variant of \LAD is called \PathLAD. It combines the locally all-different constraints
introduced by Solnon~\cite{Solnon:2010} with the exploitation of path length properties proposed by
Audemard et al.~\cite{Audemard:2014}. The idea is to label each edge $(u,v)$ with the number of
paths of length 2 between $u$ and $v$, and each node $u$ with the number of cycles of length 3
passing through $u$, and to add the constraint that the label of a pattern node (resp.\ edge) must
be smaller than or equal to the label of its associated target node (resp.\ edge).


\subsection{Problem Instances}

We consider a large benchmark set of 5725 instances, which are available in a simple text
format\footnote{\url{http://liris.cnrs.fr/csolnon/SIP.html}}. These instances are grouped into 12
classes.

\begin{itemize}
\item Class 1 contains randomly generated scale-free graphs~\cite{constraints10}.
\item Classes 2 and 3 contain instances built from a database containing
    various kinds of graph gathered by Larrosa and Valiente~\cite{LV02}: class
    2 contains small instances generated from the first 50 graphs of the
    database, and class 3 contains larger instances with pattern graphs from
    the first 50 graphs of the database and target graphs from the next 50
    graphs.
\item Classes 4 to 8 contain randomly generated graphs from a database of
    graphs commonly used for benchmarking subgraph isomorphism
    algorithms~\cite{GraphDatabase1}: bounded-degree graphs for
    classes 4 and 5, regular meshes for classes 6 and 7, and random graphs with
    uniform edge probabilities for class 8. All of these instances are
    satisfiable.
\item Classes 9 and 10 contain instances from segmented images~\cite{pr15,cviu11}.
\item Class 11 contains instances from meshes modeling 3D objects~\cite{cviu11}.
\item Class 12 contains random graph instances chosen to be close to the satisfiable-unsatisfiable
    phase transition---these instances are expected to be particularly challenging, despite their
    small size.
\end{itemize}

\noindent Note that Classes 3 and 12 were not considered in the previous
experimental study by McCreesh and Prosser~\cite{McCreesh:2015}.  This set of
instances is also much larger than that of Battiti and
Mascia~\cite{battiti-mascia07}, who were the first to propose algorithm
portfolios for subgraph isomorphism problems.  Battiti and Mascia only
considered a pure parallel portfolio consisting of two randomised solvers, and
without a selection mechanism. Their problem set consisted entirely of
satisfiable instances.

\subsection{Experimental Setup}

We measured runtimes on machines with Intel Xeon E5-2640 v2 CPUs and 64GBytes RAM, running
Scientific Linux 6.5. We used the C++ implementation of the \Glasgow algorithm~\cite{McCreesh:2015},
the C implementation of \LAD{}~\cite{Solnon:2010}, and the VFLib C++
implementation of \VFtwo{}~\cite{Cordella:2004}. Software was compiled using GCC 4.9. Each problem instance was run with a
timeout of $10^8$ milliseconds ($\approx$ 27.8 hours).


\subsection{Results}\label{expComp}

\begin{figure}[p]
    \centering
    \input{graph-cumulative}\vspace{-0.5cm}

    \caption{Number of solved instances over CPU time for the eight algorithms we
        consider in this paper, and the virtual best solver (VBS) that shows the best
    solver on an instance by instance basis.\label{expTimeGraph}}
\end{figure}

\begin{table}[p]
    \centering\setlength{\tabcolsep}{0.2em}
\begin{tabularx}{.85\textwidth}{XXrXrrrXrrrr}
\toprule
Class && \VFtwo && \multicolumn{3}{c}{\LAD} && \multicolumn{4}{c}{\Glasgow}\\
      &&&&\textsc{Incomplete}&\textsc{Default}&\textsc{Path}&&\multicolumn{1}{c}{\textsc{1}}&\multicolumn{1}{c}{\textsc{2}}&\multicolumn{1}{c}{\textsc{3}}&\multicolumn{1}{c}{\textsc{4}}\\
\midrule
1 &&        0 &&       20 &        0 &        0 &&       80 &        0 &        0 &        0 \\
2 &&      201 &&       92 &      189 &      270 &&      520 &      180 &       53 &       15 \\
3 &&      112 &&     1608 &      617 &      959 &&      396 &      195 &       21 &        0 \\
4 &&      270 &&        0 &        0 &        0 &&        5 &        0 &        0 &        0 \\
5 &&      266 &&        0 &        1 &        3 &&       31 &        0 &        0 &        0 \\
6 &&       71 &&        0 &        0 &        0 &&        7 &       14 &        1 &        0 \\
7 &&      270 &&        0 &        0 &        0 &&        5 &        0 &        0 &        0 \\
8 &&        0 &&        0 &        0 &        1 &&      195 &       69 &        6 &        0 \\
9 &&       77 &&        3 &        0 &       19 &&      103 &        1 &        0 &        0 \\
10 &&       13 &&        0 &        2 &        2 &&        7 &        0 &        0 &        0 \\
11 &&        2 &&      142 &       71 &       17 &&       23 &        0 &        0 &        0 \\
12 &&        0 &&        0 &        1 &        2 &&      158 &        6 &        1 &        0 \\
\midrule
Total && 1282 && 1865 & 881 & 1273 && 1530 & 465 & 82& 15\\
\bottomrule \\
\end{tabularx}
\caption{Number of times each algorithm is best, for each class.\label{expClass}}
\end{table}

Figure~\ref{expTimeGraph} displays the evolution of the number of instances solved with respect to
CPU time. It shows us that the best solver depends on the time limit considered. \IncompleteLAD is
able to solve easy unsatisfiable instances very quickly, in a few milliseconds. Hence, for time
limits lower than \SI{5}{\ms}, the best solver is \IncompleteLAD. However, it is not able to solve harder
unsatisfiable instances, nor can it solve satisfiable instances.

\PathLAD and \GlasgowOne outperform \IncompleteLAD for longer time limits: \PathLAD is the
best solver for time limits larger than \SI{5}{\ms} and lower than \SI{40}{\ms}, and \GlasgowOne is the best solver
for time limits larger than \SI{40}{\ms} and lower than \SI{3000}{\ms}.

\GlasgowTwo becomes the best solver for time limits larger than \SI{3000}{\ms}.
As we increase the CPU time limit, the performance of variants of \Glasgow with
longer paths (\GlasgowThree and \GlasgowFour) improves. This is what we expect,
as more reasoning is expensive, but increases the potential reduction of the
search space. Eventually, \GlasgowTwo and \GlasgowThree become very closely
matched, and with runtimes very close to the limit, \GlasgowFour nearly catches
up. (This behaviour is class-dependent: over class 2, for example, the behavior
is roughly monotone, with \GlasgowOne dominating for low runtimes, then
\GlasgowTwo, then \GlasgowThree, then \GlasgowFour each becoming best as the runtimes
increase.)

The figure illustrates the potential for portfolios and algorithm selection we have: there is
clearly no single solver that dominates throughout.

Furthermore, the virtual best solver (VBS), which considers the best algorithm
for each instance separately, obtains much better results, showing us that the
algorithms have complementary performance. The difference between VBS and single
best is particularly pronounced for CPU time limits less than \SI{1000}{\ms}. In
many applications, it is important to have the fastest possible algorithm even
if the absolute differences in CPU time are small. For example, in pattern
recognition~\cite{pr15,cviu11} and chemical~\cite{Giugno:2013} applications, we
often have to solve the subgraph isomorphism problem repeatedly for a very large
number of graphs (in order to find a pattern image or molecule in a large
database of target images or compounds, for example), so having an algorithm
that is able to solve an instance in \SI{100}{\ms} instead of \SI{1000}{\ms}
makes a big difference.  Therefore, it is important to select the best algorithm
for each instance, even if the instance is an easy one. Furthermore, this
selection process should not unduly penalise easy instances, i.e.\ it should not take more time than
the solution process time for these instances.

Table~\ref{expClass} shows us that we cannot simply select algorithms based on
the instance class. For all
classes, there are always at least two algorithms which are the best for at least one instance of
the class. In particular, for classes 2 and 3, each algorithm is the best for at least one instance
(except \GlasgowFour for class 3).

\section{Algorithm Selection Approach}\label{sec:algsel}

Our approach is composed of three steps. First, we run two presolvers in a static way to quickly
solve easy instances; second, we extract features from instances which are not solved by the first
step; and third, we select an algorithm and run it.

\subsection{Presolving}

Experimental results reported in Section~\ref{expComp} have shown us that \IncompleteLAD is very fast
(\SI{7}{\ms} on average) and is able to solve 1919 instances from our benchmark
set very quickly. Therefore, we first run
\IncompleteLAD: if unsatisfiability is detected, we do not need to process it further.

\VFtwo is also able to solve many easy instances very quickly: among the 3806 instances which are not
solved by \IncompleteLAD, 1470 are solved by \VFtwo in less than 50 milliseconds. Therefore, after
running \IncompleteLAD, we run the \VFtwo solver for \SI{50}{\ms}. This solves easy instances without
the overhead of running algorithm selection and avoids potentially making incorrect solver choices.

We include \VFtwo in the portfolio, as it may solve an instance given more time,
but not \IncompleteLAD.

After the presolving step, we are left with 2336 hard instances that we consider
for algorithm selection.

\subsection{Feature Extraction}

If presolving does not give us a solution, we extract features. For both the pattern and
the target graph, we consider some basic graph properties, which can be computed
very quickly:

\begin{itemize}
    \item the number of vertices and edges;
    \item the density---we expect that some kinds of filtering (like those based upon locally
        all-different constraints) might be expensive and ineffective on
        dense graphs;
    \item how many loops (self-adjacent vertices) the graph contains---since loops must be mapped to
        loops, this could have a strong effect on how easy an instance is;
    \item the mean and maximum degrees, and whether or not every vertex has the same degree (the
        degree-based invariants used by \LAD and \Glasgow do nothing at the top of search if every
        vertex has the same degree);
    \item whether or not the graph is connected;
    \item the mean and maximum distances between all pairs of vertices (if nearly all vertices are
        close together, path-based reasoning is likely to be ineffective) and
        the proportion of vertex pairs which are at least 2, 3 and 4 apart.
\end{itemize}

\noindent Alongside these basic features, we include information computed by \IncompleteLAD. To (try to) prove
inconsistency, \IncompleteLAD removes candidate pairs. The number of successfully removed pairs
gives information on the distribution of edges (the fewer removed pairs, the more uniform the
distribution). As well as the number of removed pairs, we also record the percentage with respect to
all possible pairs, and the minimum and maximum percentages of removed values on
a per-variable basis.

\subsection{Selection Model}

We use \LLAMA~\cite{kotthoff_llama_2013} to build our algorithm selection model.
\LLAMA supports the most common algorithm selection approaches used in the literature. We performed a
set of preliminary experiments to determine the approach that works best here.

We use 10-fold cross-validation to determine the performance of the \LLAMA models. The entire set of
instances was randomly partitioned into 10 subsets of approximately equal size. Of the 10 subsets, 9
were combined to form the training set for the algorithm selection models, which were evaluated on
the remaining subset. This process was repeated 10 times for all possible combinations of training
and test sets. At the end of this process, each problem instance in the original set was used
exactly once to evaluate the performance of the algorithm selection models.

\LLAMA's pairwise regression approach with random forest regression gave the best performance. The
idea is very similar to the pairwise classification models used by Xu et al.~\cite{xu_satzilla_2008}. For
each pair of algorithms in our portfolio, we train a model that predicts the performance difference
between them. If the first algorithm is better than the second, the difference is positive,
otherwise negative. The algorithm with the highest cumulative performance difference, i.e.\ the most
positive difference over all other algorithms, is chosen to be run.

As this approach gives very good performance already, we did not tune the parameters of the random
forest machine learning algorithm. It is possible that overall performance can be improved by doing
so and we make no claims that the particular algorithm selection approach we use in this paper
cannot be improved.

The data we use in this paper is available as ASlib~\cite{aslib} scenario GRAPHS-2015.

\section{Experimental Evaluation of Algorithm Selection}\label{sec:algsel-exps}

Table~\ref{tab:res} shows the performance of our algorithm selection approach,
compared to two baselines, on the reduced set of 2336 instances. The virtual
best solver is the oracle predictor that, for each instance, chooses the best
solver from our portfolio. This is the upper bound of what an algorithm
selection approach can achieve. The single best solver is the one solver from
the portfolio that has the overall best performance across the entire set of
instances, at the CPU time limit of \SI{e8}{\ms}, i.e.\ \GlasgowTwo. We consider
it a lower bound on the performance of the algorithm selection approach.  We are
able to solve 30 more instances than the single best solver within the timeout,
with only an additional 16 to the virtual best. In terms of average performance,
we are able to close \SI{64}{\percent} of the gap between the single best and
the virtual best solver.

Figure~\ref{fig:portfolio-ecdf} shows the number of solved instances over time
for the individual solvers, the virtual best solver, and the \LLAMA algorithm
selection approach. The algorithm selection model does not perform well for
instances that can be solved quickly because of the overhead incurred through
feature computation.  As the instances become more difficult to solve, its
performance improves.

\begin{table}[p]
    \centering\setlength{\tabcolsep}{1em}
\begin{tabular}{lrrr}
  \toprule
model & mean MCP & solved instances & mean performance\\
  \midrule
virtual best & 0 & 2219 & 5822809\\
\LLAMA & 705097 & 2203 & 6529563\\
\GlasgowTwo & 1960683 & 2173 & 7783492\\
   \bottomrule \\
\end{tabular}
\caption{Algorithm selection performance on the reduced set of 2336 instances. MCP is the
misclassification penalty; that is, the additional time required to solve an instance because of
choosing solvers that perform worse than the best. Mean MCP and performance are over all 2336 instances; when an
instance is not solved, its performance is set to the time limit
(\SI{e8}{\ms}).}\label{tab:res}
\end{table}

\begin{figure}[p]
\input{graph-reduced}

\caption{Number of solved instances over time for the virtual best solver,
\LLAMA, and the single best solver \GlasgowTwo on the reduced set of 2336
instances. Other individual solvers are shown as dotted lines, in the same
colours as Figure~\ref{expTimeGraph}. }
\label{fig:portfolio-ecdf}
\end{figure}

Table~\ref{tab:res} shows the performance of the selection model on its own. The
performance of the entire algorithm selection system, including the
preprocessing, is shown in Table~\ref{tab:resfull}. Our system is able to close
more than \SI{60}{\percent} of the gap between single and virtual best, similar
to the results on the reduced set of instances.

Figure~\ref{fig:portfolio-ecdf-full} shows the number of solved instances over
time for the algorithm selection system including \IncompleteLAD and \VFtwo
presolving on the full set of instances. The performance on small instances is
much better than the \LLAMA selector alone (cf.\
Figure~\ref{fig:portfolio-ecdf}) and the region where \LLAMA performs worse than
the individual solvers is now limited to approximately $10^2$ to $10^5$
milliseconds.

\begin{table}[p]
    \centering\setlength{\tabcolsep}{1em}
\begin{tabular}{lrrr}
  \toprule
model & mean MCP & solved instances & mean performance\\
  \midrule
virtual best & 0 & 5608 & 2375913\\
\LLAMA & 287704 & 5592 & 2664293\\
\GlasgowTwo & 798660 & 5562 & 3174573\\
   \bottomrule \\
\end{tabular}
\caption{Algorithm selection system performance on the full set of 5725
instances.}\label{tab:resfull}
\end{table}

\begin{figure}[p]
    \input{graph-full}
\caption{Number of solved instances over time for the virtual best solver,
\LLAMA, and the single best solver \GlasgowTwo on the full set of 5725
instances. Other individual solvers are shown as dotted lines, in the same
colours as Figure~\ref{expTimeGraph}. }
\label{fig:portfolio-ecdf-full}

\end{figure}

We train the algorithm selection model specifically for the timeout of $10^8$
milliseconds. In particular, we are interested in minimising the performance
difference to the virtual best. Problem instances that take longer to solve
contribute more to this difference than easy instances and therefore carry more
weight for the algorithm selection model. That is, choosing the wrong solver for
a hard instances is much worse than choosing the wrong solver for an easy
instance.

Even though Figures~\ref{fig:portfolio-ecdf} and~\ref{fig:portfolio-ecdf-full}
show that on easy instances, the model does not perform well, it does over
the entire set of instances (cf.\ Tables~\ref{tab:res} and~\ref{tab:resfull}).

\subsection{Analysis of Features Used by the Model}

Analysing the final model, we saw that the most important features were, in order:

\begin{itemize}
    \item the maximum degree of the pattern graph;
    \item the mean degree of the target graph;
    \item the proportion of target vertices that are at least distance 3
        apart;
    \item the number of values removed during \IncompleteLAD presolving.
\end{itemize}

\noindent
We introduced the proportion of target vertices that are at least 3 apart as a
feature expecting it to be helpful in distinguishing between
\Glasgow variants---if few vertices are far apart, longer paths are unlikely to
be useful. However, in practice this feature also gives a rough indication of
how sparse the graph is---locally all-different filtering is weak and expensive
on dense graphs, and the feature turned out to be helpful for selecting between
\Glasgow and \LAD variants too.

As expected, both the pattern graph and the target graph provide important
features. We also conclude that even basic graph properties are predictive of
sophisticated algorithms' performance.

\subsection{Analysis of \PathLAD versus \GlasgowTwo}

To gain further insight into the behaviour of the algorithms, we
investigated what affects the relative performance of \PathLAD and \GlasgowTwo.
This pair is of particular interest because they are the best ``medium-case''
algorithms that use strong and weak filtering during search respectively. We
used machine learning techniques to train a simple, human-understandable model
which is able to distinguish these solvers for the 2336 hard instances and gives
performance better than always choosing one of them. The model uses four rules:

\begin{enumerate}
    \item If \IncompleteLAD presolving removes at least
        \SI{28.014755}{\percent} of the pairs, and at least
        \SI{94.117645}{\percent} of the values from at least one domain, then
        pick \PathLAD.
    \item If the target has at least $610$ vertices, and if the maximum
        distance between any two pattern vertices is at most $8$, and if the
        pattern is not regular, and if the time taken to compute the
        distance-based features on the target graph is no more than
        \SI{1277}{\ms}, then pick \PathLAD.
    \item If \IncompleteLAD filtering removed at least \SI{5.904644}{\percent} of
        possible pairs, and if less than \SI{84.6565}{\percent} of the pattern
        vertices are within distance 2 of each other, then pick \PathLAD.
    \item Otherwise, pick \GlasgowTwo.
\end{enumerate}

\noindent
The first and third rules intuitively make sense: if \IncompleteLAD
filtering does well, it is likely that continuing with this kind of filtering
during search will be successful. The third rule also excludes using \PathLAD
on very dense pattern graphs, where locally all-different filtering is
expensive and weak. The second rule is less obvious: while \PathLAD filtering is
weak on regular graphs and it makes sense to exclude this case, the other
components appear to exclude large and dense target graphs. The model suggests
that it would be worth exploring \emph{dynamically} enabling or disabling
locally all-different filtering during search, based upon very simple features
which could be recomputed as search progresses and conditions change.

This provides an interesting insight into the behaviour of our algorithms, as
well as giving indications for future work.

\section{Conclusion and Future Work}\label{sec:concs}

The problem of identifying subgraph isomorphisms is a hard computation problem that has many
applications in diverse areas. In this paper, we presented a portfolio of six algorithms from the
literature and two new variants of the \LAD algorithm. We introduced a set of novel features to
characterise subgraph isomorphism problems and leveraged them to select the most appropriate
algorithm from the portfolio for each instance.

We demonstrated that our algorithm selection approach achieves substantial
performance improvements over the single algorithm that has the best performance
on our benchmark set. We showed that combining an algorithm selection approach
with a new incomplete variant of \LAD that is able to detect inconsistencies
and a presolver boosts performance even further. Finally, we showed how
insights from machine learning can guide algorithm development.

Directions for future work include scheduling multiple solvers to run instead of
a single one; in particular the \Glasgow algorithms provide a multi-core
parallel implementation, which can use a configurable number of threads. It
would also be interesting to investigate other variants of the subgraph
isomorphism problem.

\bibliographystyle{splncs}
\bibliography{paper}

\end{document}
